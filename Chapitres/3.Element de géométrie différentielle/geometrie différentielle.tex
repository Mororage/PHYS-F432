{
\renewcommand\indexmarker{\cdot}

\chapter{Éléments de géométrie différentielle}
La relativité générale repose sur le PE qui stipule que, dans des régions suffisamment restreintes de l'espace-temps, un observateur ne peut distinguer un champ de gravitation d'une accélération constante. Il lui est donc impossible de détecter l'existence d'un champ de gravitation par le biais d'expériences locales : les lois de la physique doivent se réduire dans ces régions à celles de la relativité restreinte, dont nous avons revu quelques éléments-clefs. Une conséquence directe du PE est le décalage vers le rouge (redshift) de photons s'élevant dans un champ de gravitation, ce qui a suggéré que ceux-ci se déplaçaient dans un espace-temps courbe.

Nous sommes donc amenés à décrire l'espace-temps par une structure mathématique qui ressemble localement à $\R^{1,3}$, mais qui peut posséder une \emph{courbure} non-triviale sur des régions étendues\footnote{De la même manière que le principe de relativité, et l'invariance de la vitesse de la lumière dans le vide pour tout référentiel inertiel, ont amené à unifier l'espace et le temps dans une structure commune via les transformations de Lorentz, à l'invariance de l'intervalle et à l'espace-temps de Minkowski.}

Le type d'objet capturant ici ces propriétés est une \emph{variété}, qui est l'objet d'étude d'un domaine mathématique appelée la \emph{géométrie différentielle}. 

Nous allons introduire en premier lieu la notion de \emph{tenseur} dans l'espace-temps plat (section 1), pour ensuite s'intéresser aux \emph{variétés} (section 2) qui nous permettrons de généraliser les définitions de tenseurs à des espace-temps courbes (section 3). Finalement, nous verrons à la section 4 une classe de tenseurs particulièrement utile, appelées \emph{formes différentielles}.

Une attention particulière sera portée à l'ordre des indices des matrices $\Lambda\indices{^\mu_\nu}$, qui n'est incluse que pour une raison de rigeur. Celui-ci pourra souvent être ignoré dans les calculs.
\section{Tenseurs dans un espace-temps plat}

\subsection{Espace tangent}
On se place encore dans l'espace-temps de Minkowski $\mathcal{M} \equiv \R^{1,3}$ avant de généraliser les concepts à une variété différentielle $\mathcal{M}$ arbitraire.

\begin{theoremframe}
    \begin{notat}
        Soit $P\in \mathcal{M}$. L'espace tangent à $\mathcal{M}$ au point $P$ est noté $T_P\mathcal{M}$. C'est un espace vectoriel réel de dimension 4.
    \end{notat}
\end{theoremframe}
La définition précise d'un espace tangent sera donné après avoir introduit les variétés. Les éléments de $T_P\mathcal{M}$ sont appelés \textit{vecteurs} au point $P$. 
\begin{rmk}
    La notion locale (au point $P$) de cette définition sera importante : un vecteur ne pourra pas être vu comme une flèche rejoignant le point $P$ à un autre point $Q \in \mathcal{M}$. Le vecteur \textit{ne vit pas} sur le même espace que les points $P$ et $Q$. Lorsque $\mathcal{M} = \R^{1,3}$, il n'y a pas de problème car les ensembles $\mathcal{M}$ et $T_P\mathcal{M}$ coincident. On peut alors les voir comme des flèches rejoignant deux points distincts. Ce ne sera plus le cas si l'espace-temps est courbe.
\end{rmk}
\begin{theoremframe}
    \begin{defi}
        L'union disjointe de tous les espaces tangents de $\mathcal{M}$ noté $T\mathcal{M}$ est appelé \textit{fibré tangent} de $\mathcal{M}$ :
        \begin{align}
            T\M &= \bigsqcup_P T_P\mathcal{M} \\
            & = \bigcup_{P \in \M} \{ P\} \times T_P \M \\
            & = \{ (P,V) \mid P \in \M, \, V \in T_P\M\}
        \end{align}
    \end{defi}
\end{theoremframe}
Comme $T_P\mathcal{M}$ est un espace vectoriel à 4 dimensions, il existe une base de vecteurs $\{\overline{e}_\alpha\}_\alpha$ de $T_P\mathcal{M}$ où $\alpha = \{0,1,2,3\}$. Un vecteur $V \in T_P\mathcal{M}$ peut donc être décomposé sur cette base selon $V = V^\alpha \overline{e}_\alpha$, où $V^\alpha$ sont les composantes du vecteur $V$. Remarquons donc que bien que $V$ ne dépend pas du choix de la base, $V^\alpha$ dépend bien de ce choix.
\begin{theoremframe}
    \begin{defi}
        La donnée d'un vecteur $x_P \in T_P\M$ à tout point $P \in \M$ est un champ de vecteurs :
        \begin{align}
            U : \quad & \M \longrightarrow T_P\M \\
            & P \longmapsto U(P)
        \end{align}
    \end{defi}
\end{theoremframe}
L'exemple canonique de vecteurs est le vecteur tangent à une courbe :
\begin{exmp}
    Soit une courbe lisse $\R \to \mathcal{M}: \lambda \mapsto x^\mu (\lambda)$. Le vecteur tangent à la courbe est
    \begin{equation}
        V^\alpha = \frac{\td x^\alpha}{\td \lambda}
    \end{equation}
\end{exmp}
\begin{theoremframe}
\begin{propri}
    Un vecteur $V^\alpha\in T_P\mathcal{M}$ se transforme sous transformation de Poincaré comme
    \begin{equation}
        V^{\alpha'} = \Lambda\indices{^{\alpha'}_{\mu}} \, V^\mu
    \end{equation}
\end{propri}
\end{theoremframe}
\begin{proof}
    Soit une courbe lisse $x^\mu(\lambda)$. Sous transformation de Poincaré $(\Lambda,a)$, cette courbe se transforme comme $x^{\mu'} = \Lambda\indices{^{\mu'}_{\mu}} \, x^\mu + a^{\mu'}$. Notons $V^\mu$ le vecteur tangent à la courbe au point $P$. Alors, par définition de $V$, on trouve
    \begin{align}
        V^{\mu'} = \frac{\td x^{\mu'}}{\td \lambda} = \Lambda\indices{^{\mu'}_{\mu}} \, \frac{\td x^{\mu}}{\td \lambda} = \Lambda\indices{^{\mu'}_{\mu}}\, V^\mu
    \end{align}
\end{proof}
\begin{notat}
    Dans la suite du cours, nous expliciterons des changements de référentiels par des indices (usuellement une barre dans ce syllabus), et non sur les quantités elles-mêmes. Par exemple : le vecteur $V$ peut s'écrire $V^\alpha$ dans un référentiel et $V^{\alpha'}$ dans un autre référentiel (à la place de $V'^{\alpha}$). Cette notation permet de facilement distinguer quel indice provient de quel référentiel.\footnote{Nous avons opté pour cette convention plutôt que celle des "primes" du cours magistral pour des raisons typographiques.} Il n'y a par ailleurs aucun lien entre la notation des vecteurs de base et les indices.
\end{notat}
\begin{theoremframe}
    \begin{propri}
        Sous transformation de Poincaré, les vecteurs de base se transforment comme
        \begin{equation}
            \overline{e}_{\mu'} = \Lambda\indices{_{\mu'}^{\mu}}\, \overline{e}_{\mu}
        \end{equation}
    \end{propri}
\end{theoremframe}
\begin{proof}
    La décomposition d'un vecteur dans deux référentiels inertiels différents s'écrit
    \begin{equation}
        V = V^{\mu} \, \overline{e}_{\mu} = V^{\alpha'} \,\overline{e}_{\alpha'}
    \end{equation}
    Ainsi,
    \begin{equation}
        V^{\alpha'}\, \overline{e}_{\alpha'} = \Lambda\indices{^{\alpha'}_{\mu}} \, V^\mu \,\overline{e}_{\alpha'} = V^\mu \,\overline{e}_{\mu}
    \end{equation}
    Comme ceci est valable pour $V^\mu$ arbitraire, on conclut que 
    \begin{equation}
        \label{eq:preuve loi base vecteur}
        \Lambda\indices{^{\alpha'}_{\mu}} \,\overline{e}_{\alpha'} = \overline{e}_{\mu}.
    \end{equation}
    En remarquant que $\Lambda$ vérifie\footnote{Via $\Lambda \eta \Lambda^T = \eta$.}
    \begin{equation}
        \Lambda\indices{_{\alpha}^\mu}\, \eta_{\mu\nu} \,\Lambda\indices{_{\beta}^\nu} = \eta_{\alpha\beta}
    \end{equation}
    on trouve la propriété
    \begin{equation}
        \label{eq:id lambda}
        \Lambda\indices{_{\alpha}^\mu} \, \Lambda\indices{^\beta_\mu} = \delta^\beta_{\alpha}
    \end{equation}
    Ceci permet de conclure, en multipliant l'équation \ref{eq:preuve loi base vecteur} par $\Lambda\indices{_{\alpha'}^{\mu}}$ que
    \begin{equation}
        \overline{e}_{\alpha'} = \Lambda\indices{_{\alpha'}^{\mu}}\, \overline{e}_{\mu}
    \end{equation}
\end{proof}

\subsection{Espace co-tangent}
\begin{theoremframe}
    \begin{defi}
        Une forme linéaire sur un espace vectoriel $(\mathcal{V},\mathbb{K})$ est une application linéaire $w:\mathcal{V} \to \mathbb{K}$. Pour $a,b\in \mathbb{K}$ et $X,Y \in \mathcal{V}$, on a donc
        \begin{equation}
            w(aX+bY) = aw(X)+bw(y)
        \end{equation}
    \end{defi}
\end{theoremframe}
Nous appelons \textit{dual} d'un espace vectoriel $\mathcal{V}$ l'ensemble des formes linéaires sur cet espace.
\begin{theoremframe}
    \begin{defi}
        L'espace \textit{co-tangent} à $\mathcal{M}$ en un point $P$ est le dual de $T_P\mathcal{M}$ et est noté $T_P^*\mathcal{M} = (T_P\mathcal{M)^*}$.
    \end{defi}
\end{theoremframe}
\begin{theoremframe}
    \begin{defi}
        Un \textit{covecteur} est un élément de l'espace co-tangent $w\in T_P^*\mathcal{M}$.
    \end{defi}
\end{theoremframe}
On peut définir une base $\{\Theta^\mu\}_\mu$ sur l'espace co-tangent selon l'identité
\begin{equation}
    \Theta^\mu(\overline{e}_\nu)=\delta^\mu_\nu
\end{equation}
Dans cette base, un covecteur $w\in T_P^*\mathcal{M}$ s'écrit $w = w_\alpha \theta^\alpha$. Soit $V \in T_P\mathcal{M}$:
\begin{align}
    w(V) &= w_{\alpha} \Theta^{\alpha}(V^{\mu} \overline{e}_\mu)\\
    &=w_{\alpha} V^{\mu}\Theta^{\alpha} (\overline{e}_{\mu})\\
    &= w_{\alpha} V^{\mu}\delta^{\alpha}_{\mu}\\
    &= w_{\alpha} V^{\alpha} \in \R
\end{align}
Cette expression ne dépend pas du choix de la base. De plus, elle ne comporte pas d'indices libres. Un tel terme est appelé \emph{scalaire de Lorentz}. On peut l'écrire de manière équivalente comme $w(V) = w_{\alpha} V^{\alpha} = w_{\alpha '} V^{\alpha '} $.
\begin{rmk}
    \label{rmk:tangent^2}
    Dans un espace vectoriel de dimension finie, le dual de l'espace dual est identifié à $T_P\mathcal{M}$. On a donc $(T^*_P\mathcal{M})^* = T_P\mathcal{M}$.
\end{rmk} 
\begin{theoremframe}
    \begin{propri}
        Soit un covecteur $w_\alpha\in T^*_P\mathcal{M}$. Sous transformation de Poincaré, ce covecteur se transforme comme
    \begin{align}
        w_{\alpha'} = \Lambda\indices{_{\alpha'}^{\mu}} \, w_\mu
    \end{align}
    \end{propri}
\end{theoremframe}
\begin{proof}
    On vient de voir que $w(V)$ est un scalaire de Lorentz:
    \begin{equation}
        w(V) = w_{\alpha} V^{\alpha} = w_{\alpha'} V^{\alpha'} 
    \end{equation}
    Et donc par la loi de transformation d'un vecteur, on a
    \begin{equation}
        w_{\alpha} V^{\alpha} = w_{\alpha'}\, \Lambda\indices{^{\alpha'}_{\mu}} V^\mu
    \end{equation}
    Comme ceci est valable pour tout vecteur $V$, on peut écrire
    \begin{equation}
        w_{\alpha}  = w_{\alpha'} \,\Lambda\indices{^{\alpha'}_{\alpha}}
    \end{equation}
    Et on trouve le résultat recherché en utilisant l'identité \ref{eq:id lambda}.
\end{proof}

En particulier, la base de l'espace co-tangent se transforme comme:
\begin{equation}
    \Theta^{\alpha'} = \Lambda\indices{^{\alpha'}_{\mu}}\,  \Theta^{\mu}
\end{equation}
\begin{theoremframe}
    \begin{defi}
        L'union (disjointe) de tous les espaces co-tangents de $\mathcal{M}$ noté $\bigsqcup_P T^*_P\mathcal{M}$ est appelé \textit{fibré co-tangent} de $\mathcal{M}$.
    \end{defi}
\end{theoremframe}
}
\subsection{Les tenseurs}

On peut généraliser les notions de vecteur et covecteur à un tenseur. 
\begin{theoremframe}
    \begin{defi}
        Un tenseur de type $(k, l)$ au point $P$ est une application multilinéaire
        \begin{equation}
            \underbrace{T^*_{P}\mathcal{M} \times \cdots \times T^*_{P}\mathcal{M}}_\text{$k$-fois} \times \underbrace{T_{P}\mathcal{M} \times \cdots \times T_{P}\mathcal{M}}_\text{$l$-fois}  \to \mathbb{R}
        \end{equation}

        Multilinéaire signifie linéarité en chaque variable, donc par exemple pour un tenseur de type $(1,1)$: 
        \begin{equation}
        T(aw +b\eta, cX +dY) = ac \, T(w, X) + ad \, T(w, Y) + bc \, T(\eta, X) +cd \, T(\eta, Y)
        \end{equation}
        où $a,b, c ,d \in \mathbb{R}$
    \end{defi}
\end{theoremframe}
\begin{exmp}
    Si on considère un vecteur comme un élément du duel de l'espace cotangent (équivalent d'après la remarque \ref{rmk:tangent^2}) :
\begin{align}
    V \in T_{p}\mathcal{M} : \quad &T^*_{p}\mathcal{M} \longrightarrow \mathbb{R}\\
    & \omega \longmapsto V(\omega)
\end{align}
alors on pourra voir le vecteur $V$ comme un tenseur $(1, 0)$. Similairement, on pourra voir le covecteur $w$ comme un tenseur $(0, 1)$. 
\end{exmp}
\begin{exmp}
    Un tenseur $(1, 1)$ est tel que $T(w,V) \in \R$ où $w$ est un covecteur et $V$ est un vecteur. 
\end{exmp}
\begin{exmp}
    Si un tenseur $T$ est tel que pour $V\in T_p\mathcal{M}$, on a $T(V) \in T^*_p\mathcal{M}$, alors il s'agit d'un tenseur de type (0,2).
\end{exmp}
\begin{theoremframe}
    \begin{propri}
        L'ensemble des tenseurs de type $(k, l)$ en $P\in \mathcal{M}$ forme un espace vectoriel. Une base de cette espace vectoriel est donnée par 
        \begin{equation}
        \overline{e}_{\alpha_1} \otimes \cdots \otimes \overline{e}_{\alpha_k} \otimes \Theta^{\mu _1}\otimes \cdots \otimes \Theta^{\mu _l}
        \end{equation}
    \end{propri}
\end{theoremframe}
Un tenseur de type $(k,l)$ se décompose donc sur cette base comme
\begin{equation}
    T = T\indices{^{\alpha _1 \cdots \alpha _k}_{\nu_1 \cdots \nu_l}}\overline{e}_{\alpha_1} \otimes \cdots \otimes \overline{e}_{\alpha_k} \otimes \Theta^{\nu_1}\otimes \cdots \otimes \Theta^{\nu_l}
\end{equation}
\begin{theoremframe}
    \begin{propri}
        Un vecteur de type $(k,l)$ se transforme sous transformation de Poincaré comme\footnote{Ici, $\times$ est utilisé pour la multiplication usuelle et non comme produit direct.}
        \begin{equation}
            T\indices{^{{\alpha}_1' \cdots {\alpha}_k'}_{{\nu_1'} \cdots {\nu_l'}}} = \Lambda\indices{^{\alpha_1'}_{\!\alpha _1 }} \times \cdots \times \Lambda\indices{^{\alpha_k'}_{\!\alpha_k}} \times \Lambda\indices{_{\nu_1' }^{\!\nu_1 }} \times \cdots \times \Lambda\indices{_{\nu_k'}^{\!\nu_k}} \times T\indices{^{\alpha _1 \cdots \alpha _k}_{\nu_1 \cdots \nu_l}}.
        \end{equation}
        La transformation implique donc $k+l$ matrices de Lorentz.
    \end{propri}
\end{theoremframe}
\begin{proof}
    Conséquence des propriétés précédentes.
\end{proof}

Revenons à la métrique de Minkowski. Celle-ci peut-être vue comme un tenseur $(2.0)$ symétrique.
{
\renewcommand\indexmarker{\cdot}
\begin{equation*}
    \eta_{\mu'\nu'} = \Lambda\indices{_{\mu'}^{\alpha}}\, \Lambda\indices{_{\nu'}^{\beta}}\,\eta_{\alpha \beta}
\end{equation*}
}
De plus, la métrique de Minkowski est un invariant de Lorentz : elle a les mêmes composantes dans tout référentiel inertiel.

\begin{theoremframe}
    \begin{defi}
        La métrique de Minkowski est un pseudo-produit scalaire pour l'espace $T_p\mathcal{M}$. En effet, celle-ci est une forme linéaire 
        \begin{align}
        \eta:\begin{pmatrix}
             T_p\mathcal{M} \times T_p\mathcal{M} & \to & \R\\
            (X,Y) & \mapsto & \eta(X,Y) = \eta_{\mu  \nu }X^{\mu}Y^{\nu}
        \end{pmatrix}.
        \end{align}
        En particulier, $\eta$ est symétrique et biliéaire.
    \end{defi}   
\end{theoremframe}
\begin{rmk}
    Elle définit un pseudo-produit scalaire car elle n'est ni définie positive $\eta(X,X) \ngeq 0$, ni sépare les points $\eta(X,X) = 0 \centernot \implies X = 0$.
\end{rmk}
On dira que deux vecteurs sont orthogonaux si et seulement si $\eta (X, Y) = 0$. On note la norme d'un vecteur
\begin{align*}
    \lVert V \rVert ^2 = \eta (V, V) = \left\{
\begin{array}{l}
  < 0 \text{ si } V \text{est de genre temps} \\
  > 0 \text{ si } V \text{est de genre espace}\\
  = 0 \text{ si } V \text{est de genre lumière}
\end{array}
\right.
\end{align*}

La métrique permet d'établir un isomorphisme entre $T_{p}\mathcal{M} $ et $T^*_{p}\mathcal{M}$. C'est-à-dire qu'il permet d'associer un vecteur à un covecteur tel que
\begin{align*}
    \eta : \begin{pmatrix}
        T_{p}\mathcal{M} &\to & T^*_{p}\mathcal{M}\\
             V &\mapsto &\eta (V, \cdot\,)
    \end{pmatrix}
\end{align*}
Ceci justifie la convention que $\eta$ permet de monter ou descendre les indices tel que:
\begin{equation*}
    V^{\mu} \rightarrow \eta_{\mu \nu}V^{\mu} \equiv V_{\nu}
\end{equation*}
En effet, par les propriétés d'un isomorphisme, un et un seul covecteur $V_\mu$ sera associé à tout vecteur $V^\mu$ (et vice-versa).
\subsection{Avantage de la notation tensorielle}
Outre le fait que la notation tensorielle permet une formulation plus compacte, on peut justifier son importance par la propriété fondamentale suivante :
\begin{theoremframe}
    \begin{propri}
        Si une relation tensorielle est valable dans un référentiel interiel alors elle sera valable dans tout référentiel inertiel.
    \end{propri}
\end{theoremframe}
Illustrons cette propriété avec les équations de Maxwell. Ces derniers sont des invariants de Lorentz mais c'est compliqué de le montrer explicitement (surtout pour les boosts). Une manière plus simple de le montrer est de les écrire en notation tensorielle, dans lesquelles elles seront nécessairement invariantes.

Les équations de Maxwell sont:
\begin{align}
    \begin{dcases}
        \vect{\nabla} \times \vect{B} -\partial_{t} \vect{E} = \vect{J}  \\
        \vect{\nabla} \cdot \vect{E} = \rho\\
        \vect{\nabla} \cdot \vect{B} = 0\\
        \vect{\nabla} \times \partial_t \vect{B} = \vect{0}
    \end{dcases}
\end{align}
L'objet qui nous permettra d'écrire ces équations sous forme tensorielle est le tenseur de Faraday :
\begin{align}
    \label{eq: Faraday}
    F_{\mu \nu}^{(-+++)} =  \begin{pmatrix}
    0 & -E_1 & -E_2 & -E_3\\
    E_1 & 0 & B_3 & -B_2\\
    E_2 & -B_3 & 0 & B_1\\
    E_3 & B_2 & -B_1 & 0
    \end{pmatrix} \quad \iff \quad \begin{dcases}
        F^{0i} = E^i\\
        F^{ij} = - \varepsilon^{ijk}B_k
    \end{dcases}
\end{align}
Ce tenseur est anti-symétrique $F_{\mu \nu} = - F_{\nu \mu}$. Introduisons les notations de \emph{symétrisation} et \emph{anti-symétrisation} :
\begin{theoremframe}
    \begin{defi}
        La \emph{symétrisation} d'un tenseur $T$ s'écrit :
        \begin{equation}
            T_{(\mu_{1} \cdots \mu_{n})} =\frac{1}{n!}\sum_{\sigma \in S(n)} T_{\sigma (\mu_{1} \cdots \mu_{n})}
        \end{equation}
        L'\emph{anti-symétrisation} d'un tenseur $T$ s'écrit :
        \begin{equation}
            T_{[\mu_{1} \cdots \mu_{n}]} =\frac{1}{n!}\sum_{\sigma \in S(n)} \sgn (\sigma) T_{\sigma (\mu_{1} \cdots \mu_{n})}
        \end{equation}
    \end{defi}
\end{theoremframe}
où $S_n$ est le groupe des permutations à r-éléments. Ce groupe contient $n!$ éléments. $\sgn(\sigma)$ est le signe de la permutation $\sigma$, c'est à dire $(-1)^k$ où $k$ est le nombre de permutations nécessaire pour retrouver l'identité. Pour en donner, un exemple simple :
\begin{equation*}
    \mathrm{Id_n} = 
    \begin{pmatrix}
        1 & 2 & 3 & \cdots & n \\
        1 & 2 & 3 & \cdots & n 
    \end{pmatrix}
\end{equation*}

En comparant à la permutation suivante :
\begin{equation*}
    s = 
    \begin{pmatrix}
        1 & 2 & 3 & \cdots & n \\
        2 & 1 & 3 & \cdots & n 
    \end{pmatrix}
\end{equation*}

Alors le signe de $s$ vaut $-1$.

Un élément général de $S_n$ s'écrite 
\begin{equation*}
\sigma = \begin{pmatrix}
1 & 2 & 3 & \cdots & n \\
\sigma(1) & \sigma(2) & \sigma(3) & \cdots & \sigma(n) 
\end{pmatrix}
\end{equation*}
\begin{exmp}
    Pour un tenseur à deux indices : 
    \begin{equation*}
        T_{[\alpha \beta]} = \frac{1}{2!}\left(T_{\alpha\beta} - T_{ \beta \alpha}\right)
    \end{equation*}
\end{exmp}
Notons qu'une (anti-)symétrisation partielle est également possible. Nous écrirons ainsi par exemple $T_{\mu [\nu \rho]}$. Notons également qu'il est nécessaire que tous les indices (anti-)symétrisés soient soit contravariant soit covariants (non-mixte).
\begin{theoremframe}
    \begin{propri}
        Les équations de Maxwell s'écrivent sous une forme tensorielle comme :
        \begin{align}
            \boxed{\begin{dcases}
                \partial_{\mu} F^{\mu \nu} = J^{\nu}(x) \\
                \partial_{[{\alpha}}F_{\mu \nu]} = 0
            \end{dcases}}
        \end{align}
        où $J^{\mu} = (\rho, \Vec{J})$ est le quadri-courant.
    \end{propri}
\end{theoremframe}
\begin{proof}
    Nous allons montrer que la loi de Gauss se réécrit comme une partie de cette formulation des équations de Maxwell. Les autres équations sont laissées comme exercices au lecteur. La loi de Gauss est
    \begin{equation}
        \partial_{i}E^{i} = J^{0}
        \label{loi de Gauss}
    \end{equation}
    \begin{rmk}
        On peut monter et descendre les indices spatiaux sans conséquences $E^{i} = \eta^{ij}E_{j} = E_{i}$.
    \end{rmk}
    Par définition du tenseur de Faraday, on a
    \begin{equation}
        F_{0i} = -E_i
    \end{equation}
    Remarquons que alors
    \begin{equation}
        F^{0i} = \eta ^{0 \alpha} \eta ^{i\beta} F_{\alpha \beta} = (-1)(+1)F_{0i} = -F_{0i} = E_i
        \label{Faraday}
    \end{equation}
    En remplaçant ceci dans \ref{loi de Gauss}, on obtient:
    \begin{equation}
        \partial_{i}F^{0i} = J^0
        \label{Loi de Gauss tensoirel}
    \end{equation}
    Comme $F^{\mu \nu}$ est antisymétrique, on a que $F^{00} = 0$. On peut donc réécrire \ref{Loi de Gauss tensoirel} comme
    \begin{equation}
        \partial_{\alpha}F^{0\alpha} = J^0
    \end{equation}
    De manière similaire, il est possible d'y intégrer la loi d'Ampère pour donner
    \begin{equation}
        \label{eq:faraday tot}
        \partial_{\mu}F^{\mu\nu} = J^{\nu}
    \end{equation}
    \cutebreak
    {
    \renewcommand\indexmarker{\cdot}
    Montrons que cette expression ne dépend effectivement pas du référentiel inertiel choisi. On sait que $x'^{\mu} = \Lambda\indices{^{\mu'}_{\alpha}}\,x^{\alpha}$. \ref{eq:faraday tot} peut donc se réécrire comme:
    \begin{equation}
        \Lambda\indices{_{\mu}^{\nu '}}\, \partial_{\nu '}(\Lambda\indices{^{\mu}_{\alpha'}}\,\Lambda\indices{^{\nu}_{\beta '}}\,F^{\alpha ' \beta '} ) =  \Lambda\indices{^{\nu }_{\alpha'}}\, J^{\alpha '}
    \end{equation}
    par la loi de transformation tensorielle. Comme les matrices $\Lambda ^{\mu}_{\alpha'}$ et $\Lambda ^{\nu}_{\beta '}$ sont constantes, on peut les sortir de la dérivée. 
    \begin{align}
        \overbrace{\Lambda\indices{_\mu^{\nu '}}\, \Lambda\indices{^{\mu}_{\alpha'}}}^{\displaystyle\delta^{\nu '}_{\alpha' }}\Lambda\indices{^{\nu}_{\beta '}}\, \partial_{\nu '} F^{\alpha ' \beta '} & =  \Lambda\indices{^{\nu }_{\alpha'}}\,J^{\alpha '} \\
        \implies \Lambda\indices{^{\nu}_{\beta '}}\, \partial_{\nu '} F^{\nu ' \beta '}  &=  \Lambda\indices{^{\nu }_{\alpha'}}\, J^{\alpha '}
    \end{align}
    Et en multipliant des deux côtés par $\displaystyle\Lambda\indices{_\nu^{\gamma'}}$, on obtient bien
    \begin{equation}
        \partial_{\alpha '}F^{\alpha ' \gamma '} = J^{\gamma '}
    \end{equation}
    Ceci montre bien que dans l'exemple des lois de Maxwell, l'expression tensorielle dans un système inertiel est en effet valable pour tout référentiel.
    }
\end{proof}
\section{Variétés différentielles}
Nous allons à présent considérer un espace-temps $\mathcal{M}$ avec courbure. Introduisons d'abord quelques notions périphériques, pour ensuite définir les \emph{variétés différentielles}. Nous verrons au chapitre suivant que ce formalisme abordera effectivement à une notion de courbure.
\begin{theoremframe}
    \begin{defi}
        Soit $X$, un ensemble non-vide. Une collection $\mathcal{T}_X$ de sous-ensembles de $X$ est une \textit{topologie sur $X$} si
        \begin{enumerate}[label = \roman*.]
            \item $X,\empty \in \mathcal{T}_X$,
            \item Si $U_1,\cdots,U_k \in \mathcal{T}_X$, alors $U_1 \cap \cdots \cap U_k \in \mathcal{T}_X$,
            \item Si $\{ U_i\}_{i\in I}$ est une collection quelconque d'éléments de $\mathcal{T}_X$, alors $\bigcup_{i\in I} U_i \in \mathcal{T}_X$.
        \end{enumerate}
        Le couple $(X,\mathcal{T}_X)$ est un \textit{espace topologique}. Les éléments de $U\in \mathcal{T}_X$ sont les ouverts de l'espace topologique.
    \end{defi}
\end{theoremframe}
Sur base de cette définition, il est possible de reconstruire les notions d'analyse comme la continuité et la différentiabilité comme approche alternative à la notion séquentielle (via des suites). Nous n'allons pas aller en détail dans cette approche et directement passer au sujet de la géométrie différentielle.
\begin{theoremframe}
    \begin{defi}
        Une application sur des ouverts $f:U\to V$ est un \emph{homéomorphisme} si
        \begin{enumerate}[label = \roman*.]
            \item $f:U\to V$ est bijective,
            \item $f$ est continue,
            \item $f^{-1}$ est continue.
        \end{enumerate} 
    \end{defi}
\end{theoremframe}
Notons que nous considérerons dans la suite que l'homéomorphisme sera lisse (c'est-à-dire $C^\infty$). L'utilité d'un homéomorphisme réside dans le fait qu'il préserve la structure topologique des espaces : une tasse de café et un donut sont homéomorphes, car l'un peut être transformé en l'autre continûment, sans coupures. 
\begin{theoremframe}
    \begin{defi}
        Un \textit{atlas} (lisse) d'un espace topologique $(X,\mathcal{T}_X)$ est une collection de paires $(U_i,\varphi_i)_{i\in I}$ appelées \emph{cartes locales} telle que :
        \begin{enumerate}[label=\roman*.]
            \item $\forall i\in I$, $U_i \in \mathcal{T}_X$ et de plus, $\bigcup_{i\in I} U_i = X$.
            \item $\varphi_i:U_i\to V_i$ où $V_i$ est un ouvert de $\R^m$ est un homéomorphisme.
            \item Si $U_i \cap U_j \neq \varnothing$, l'application 
            \begin{align}
                \varphi_{ij} \equiv \varphi_i \circ \varphi^{-1}_j:\varphi_j(U_i\cap U_j) \subset \R^m \to \varphi_i(U_i\cap U_j) \subset \R^m 
            \end{align}
            est lisse. Ces fonctions sont également appelées \emph{fonctions de transition}.
        \end{enumerate}
    \end{defi}
\end{theoremframe}
\begin{theoremframe}
    \begin{defi}
        Une variété différentielle $\mathcal{M}$ de dimension $m$ est un espace topologique muni d'un atlas.
    \end{defi}
\end{theoremframe}
\begin{rmk} 
    \begin{enumerate}
        \item[\,] 
        \item L'application $\varphi_i^{-1}$ est appelée \emph{paramétrisation locale} de $\mathcal{M}$ sur $U_i\in \mathcal{M}$. Elle permet d'associer des coordonnées de $\R^m$ à chaque point de $U_i$. On dira d'une variété $\mathcal{M}$ qu'elle ressemble localement à des ouverts de $\R^m$.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.3\linewidth]{Chapitres/3.Element de géométrie différentielle/Images/variété.jpg}
            \caption{}
            \label{fig:3.1}
        \end{figure}
        \item La condition iii. impose que ces morceaux de $\R^m$ peuvent être "recollés" de manière lisse.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\linewidth]{Chapitres/3.Element de géométrie différentielle/Images/ouvertchevauchement.jpg}
            \caption{Sur les images de l'intersection de $U_i$ et $U_j$, il existe une bijection lisse.}
            \label{fig:3.2}
        \end{figure}
        \item Un atlas sur une variété n'est pas unique. Les différents choix d'atlas correspond aux différents choix physique de système de coordonnées. La liberté de changer d'atlas correspond à la liberté de changer de coordonnées ou de référentiel pour décrire une variété.
    \end{enumerate}
\end{rmk}
    \begin{exerc}
        Montrez qu'un cône n'est pas une variété lisse. Pour rappel, le cône est défini comme 
        \begin{equation}
            C = \ltc (x,y,z)\in \R^3 \mid x^2+y^2-z^2=0, z\geq 0 \rtc
        \end{equation}
    \end{exerc}
\subsection{Applications entre variétés différentielles}
Soient deux variétés différentielles $\mathcal{M}$ et $ \mathcal{N}$ respectivement à cartes locales $(U,\varphi)$ et $(V,\psi)$ et une application continue $f:\mathcal{M} \to \mathcal{N}$.

\begin{theoremframe}
    \begin{defi}
        L'application $f$ est dite \emph{lisse} si et seulement si l'application 
        \begin{equation}
        F \equiv \psi \circ f \circ \varphi^{-1}:\varphi(U) \to \psi(V)
        \end{equation}
        est lisse. Cette fonction est appelée \emph{représentation de coordonnées locales de} $f$.
    \end{defi}
\end{theoremframe}

\begin{center}
\includegraphics[scale=0.1]{Chapitres/3.Element de géométrie différentielle/Images/fonction de transistion.jpg}
%\captionof{figure}{}
%\label{Parabolique 2}
\end{center}

\subsection{Difféomorphismes}
Soient $\mathcal{M}$ et $\mathcal{N}$ deux variétés différentielles. 
\begin{theoremframe}
    \begin{defi}
        Une application $f:\mathcal{M} \to \mathcal{N}$ est un \emph{difféomorphisme} si 
        \begin{enumerate}[label = \roman*.]
            \item $f$ est bijective,
            \item $f$ est lisse,
            \item $f^{-1}$ est lisse.
        \end{enumerate}
    \end{defi}
\end{theoremframe}
De manière équivalente, $f$ est un difféomorphisme si et seulement s'il existe une application lisse $g: \mathcal{N} \to \mathcal{M}$ telle que 
\begin{equation}
    f \circ g = \mathrm{Id}_\mathcal{N} \quad \text{et} \quad g \circ f = \mathrm{Id}_\mathcal{M}
\end{equation}
Lorsqu'il existe un difféomorphisme entre $\mathcal{M}$ et $\mathcal{N}$, on dit qu'ils sont \emph{difféomorphes} (ou \emph{équivalentes}) : on peut les déformer l'un dans l'autre de manière lisse. Un difféomorphisme va, contrairement à l'homéomorphisme, préserver également la structure différentielle et non seulement la structure topologique de la variété. Ainsi, un cube est homéomorphe mais pas difféomorphe à la sphère, car on ne pourra pas se débarrasser des "coins" de manière lisse.

\begin{exmp}
    Le cercle $S^1$ et l'ellipse $E$ sont définis par
    \begin{align}
        S^1 &\equiv \ltc (x,y) \in \R^2 \mid x^2+y^2 = 1\rtc \\
        E &\equiv \ltc (x,y) \in \R^2 \mid \lt \frac{x}{a} \rt^2 + \lt \frac{y}{b} \rt^2 = 1\rtc
    \end{align}
    On peut passer de l'un a l'autre via le changement de variable $(x, y) \mapsto (\frac{x}{a}, \frac{y}{b})$, qui est linéaire et donc lisse. 
\end{exmp}

L'ensemble des difféomorphisme d'une variété sur elle-même $f: \mathcal{M} \to \mathcal{M}$ représente l'ensemble des choix et changements de coordonnées pour décrire cette variété. La transformation au niveau des coordonnées est de la forme $x^{\mu}(P) \mapsto x^{\mu'}(x^\mu(P))$ $\forall P \in \mathcal{M}$. 

\subsection{Interprétation physique des changements de coordonnées}

Les difféomorphismes (et donc les changements de coordonnées) sur $\mathcal{M}$ peuvent être vus de 2 manières différentes:

\begin{enumerate}
    \item \textbf{Transformation active des coordonnées}\\Soit une carte locale $(U,\varphi)$ incluant un point $P$ et notons $\varphi(P) = x^\mu(P) \in \R^m$ (on dit que $x^\mu$ sont les \emph{coordonnées} de $P$ de la carte locale). Un difféomorphisme $f:\mathcal{M} \to \mathcal{M}$ envoie $P$ sur $f(P)$, et on notera $\varphi(f(P)) \equiv y^\mu(P) = x^\mu(f(P))$. La dénomination d'\emph{actif} origine du fait que le point $P$ change explicitement (en $f(P)$).
    \begin{center}
        \includegraphics[scale=0.15]{Chapitres/3.Element de géométrie différentielle/Images/transformation actives.jpg}
        %\captionof{figure}{}
        %\label{Parabolique 2}
    \end{center}
\item \textbf{Transformation passive des coordonnées}\\
Soient deux cartes locales de $P$, notées $(U,\varphi)$ et $(V,\psi)$ et notons $\varphi(P) = x^\mu(P)$ et $\psi(P) = y^\mu(P)$. Remarquons que ces deux coordonnées ne coïncident pas nécessairement. Ils sont liés par la fonction de transition $f = \varphi \circ \psi^{-1}, \, y^\mu \mapsto x^\mu$, qui est un difféomorphisme. Cette représentation est nommée \emph{passive}, car le point $P$ reste le même : la différence vient des deux cartes distinctes utilisées.
\end{enumerate}

\subsection{Un atlas pour $S^2$}
Une carte est dite \emph{globale} si elle forme un atlas à elle seule. Il s'avère que plus souvent qu'on pourrait croire, une carte globale n'existe pas. Nous illustrerons ici un cas tel où une carte globale n'existe pas : la sphère usuelle (appelée 2-sphère\footnote{Dans le cas ci-présent, bien que la sphère est un objet "plongé" dans un espace 3-dimensionnel, la sphère elle-même est une surface 2-dimensionnelle.}).
\begin{equation}
    S^2 \equiv \{ (x,y,z) \in \R^3 \mid x^2+y^2+z^2 = 1\}
\end{equation}
Nous allons construire un atlas pour $S^2$. Pour ce, nous utiliserons la \emph{projection stéréographique} : dénotons le pôle nord par $N = (0,0,1)$ et le pôle sud par $S = (0,0,-1)$. Pour tout point $P\in S^2\backslash \{N\}$, il existe une unique droite passant par $P$ et $N$. La projection stéréographique par rapport à $N$, $\varphi_N:S^2\backslash\{N\} \to \R^2$ envoie $P$ sur l'intersection de cette droite avec le plan $z=0$. Selon cette définition, le pôle sud $S = (0,0,-1)$ est envoyé sur $(0,0)$, et lorsque $P \to N$, $\lVert \varphi(P) \lVert \to \infty$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{Chapitres/3.Element de géométrie différentielle/Images/sphère S^2.jpg}
    \caption{Visualisation de la projection stéréographique par rapport à $N$.}
    \label{fig:stereographique 1}
\end{figure}
Dérivons la forme explicite de $\varphi_N$. Dénotons l'origine par $O = (0,0,0)$ et considérons un point $P = (x_P^1,x_P^2,x_P^3)\in S^2$ ainsi que sa projection stéréographique $Q \in \R^2 \times \{0\}$. Notons de plus $P_z$ la projection de $P$ sur l'axe $Oz$ et $P_{xy}$ la projection de $P$ sur le plan $Oxy$. Remarquons que les triangles $PP_zN$ et $QON$ sont semblables\footnote{Mêmes angles, mais côtés de longueurs différentes.} et donc
\begin{align}
    \frac{OQ}{ON} = \frac{P_zP}{NP_z} \implies OQ = \frac{OP_{xy}}{1-x^3_P}
\end{align}
car $ON = 1$, $P_zP = OP_{xy}$ et $NP_z = 1 - x^3_P$. On trouve alors que les composantes de la projection stéréographique sont (pour $i=1,2$) :
\begin{equation}
    Q^i = \varphi_N(P)^i = \frac{x^i_P}{1-x^3_P}
\end{equation}
ou de manière équivalente
\begin{align}
\label{eq:stereoN}
    \varphi_N : \quad S^2\backslash\{N\} &\longrightarrow \R^2 \\ 
    (x^1,x^2,x^3) &\longmapsto \frac{1}{1-x^3} (x^1,x^2)
\end{align}
Similairement, la projection stéréographique par rapport au pôle sud est
\begin{align}
    \label{eq:stereoS}
    \varphi_S: \quad S^2\backslash \{S\} &\longrightarrow \R^2 \\
    (y^1,y^2,y^3) &\longmapsto \frac{1}{1+y^3} (y^1,y^2)
\end{align}
Montrons à présent que $\{ (S^2\backslash\{N\}, \varphi_N), (S^2\backslash\{S\}, \varphi_S)\}$ est un atlas lisse de $S^2$. Pour ce, nous devons principalement montrer que les fonctions de transitions sont lisses\footnote{En effet, l'atlas couvre $S^2$ et les cartes locales sont des homéomorphismes (exercice).}. On cherche à calculer
\begin{equation}
    \varphi_S \circ \varphi_N^{-1} : \quad \R^2_0 \longrightarrow \R^2
\end{equation}
Dénotant les paramétrisations locales $\varphi^{-1}_N \equiv \psi_N$, nous allons déduire sa forme. Soit $\varphi_N^i = z^i$. Alors :
\begin{equation}
\label{eq:1 altasS2}
    z^i = \frac{x^i}{1-x^3}
\end{equation}
Et, en utilisant la contrainte de $S^2$ :
\begin{equation}
    (z^1)^2 + (z^2)^2 = \frac{1 - (x^3)^2}{(1-x^3)^2} = \frac{1 + x^3}{1-x^3}
\end{equation}
Ceci nous permet d'isoler et de réinjecter $x^3$ dans (\ref{eq:1 altasS2}) :
\begin{align}
    \psi_N = \varphi_N^{-1}: \quad \R^2 &\longrightarrow S^2\backslash \{N\}\\
    (u,v) &\longmapsto \frac{1}{1+u^2 +v^2} \lt 2 u,2v, -1 + u^2+v^2 \rt
\end{align}
La fonction de transition est alors
\begin{align}
    \varphi_S \circ \psi_N : \psi^{-1}_N \circ \varphi^{-1}_S :\quad \R^2_0 &\longrightarrow \R^2\\
    (u,v) &\longmapsto \frac{(u,v)}{u^2+v^2}
\end{align}
Qui est lisse sur $\R^2_0$. Remarquons que $\psi_N(0,0) = (0,0,-1)=S$, donc notre domaine de définition est bien correct (comme $\varphi_S$ n'est pas définie en $S$, nous devons exclure $(0,0)$ du domaine). Il s'agit ici d'une subtilité importante de la condition dans la définition d'un atlas : il faut être lisse seulement sur l'intersection de leurs domaines (excluant ici $(0,0)$, qui autrement aurait posé problème).
\begin{exerc}
    Dérivez l'expression (\ref{eq:stereoS}) pour $\varphi_S : S^2\backslash \{ S\} \to \R^2$. Calculez $\psi_S$ et montrez que $\varphi_N \circ \varphi_S^{-1}$ est bien lisse sur son domaine. 
\end{exerc}
Nous avons donc trouvé un atlas lisse de $S^2$.
\begin{exerc}
    Montrez qu'un atlas global de $S^2$ n'existe pas (indice : utiliser la compacticité de la sphère).
\end{exerc}
\section{Variétés différentielles et tenseurs}
Nous devons retravailler la notion de tenseur introduite au chapitre précédent pour un espace-temps plat, afin qu'elle soit \emph{intrinsèque} à $\mathcal{M}$ : une variété arbitraire n'est pas un espace vectoriel et un vecteur ne peut donc être défini sur l'espace lui-même, il faut passer par les espaces tangents. Fixons un point $m\in \mathcal{M}$ (disons, à $\dim \mathcal{M} = n$) et une courbe $\gamma$ dans $\mathcal{M}$ telle que $\gamma(0)=m$.
\begin{theoremframe}
    \begin{defi}
        Un \emph{vecteur tangent} $X_m$ à $\mathcal{M}$ en $m$ est un opérateur différentiel linéaire agissant sur les fonctions lisses dans un voisinage de $m$ tel que
        \begin{equation}
        \label{eq: vectvar}
            X_m : f \mapsto X_m f =\left. \frac{\td}{\td \lambda}\right|_{\lambda=0}  f \circ \gamma(\lambda)
        \end{equation}
    \end{defi}
\end{theoremframe}
Un vecteur tangent peut donc être vu comme une dérivée directionnelle s'appliquant sur les fonctions le long d'une courbe en $\lambda=0$. Nous abuserons parfois de cette notation en écrivant $X = \frac{\td}{\td \lambda}$. Notons que (\ref{eq: vectvar}) s'écrit en composantes :
\begin{equation}
    X_i f = \left. \frac{\td}{\td \lambda}\right|_{\lambda=0}  f \circ \gamma_i(\lambda).
\end{equation}
Soit une carte locale $(U,\varphi)$ de $\mathcal{M}$ au point $m$. Notons $F = f \circ \varphi^{-1} : \R^m\to \R$ et $x^\mu(\gamma) = \varphi \circ \gamma: \R \to \R^m$ les représentations en coordonnées de $f$ et $\gamma$ dans cette carte locale. On trouve alors
\begin{align}
    X_m f = \left. \frac{\td}{\td \lambda}\right|_{\lambda=0}  f \circ \gamma(\lambda) &= \left. \frac{\td}{\td \lambda}\right|_{\lambda=0}  f \circ \varphi^{-1} \circ \varphi \circ \gamma(\lambda)\\
    & = \left. \frac{\td}{\td \lambda}\right|_{\lambda=0}  F \circ x^\mu(\gamma((\lambda))
\end{align}
S'agissant à présent de fonctions dans des espaces euclidiens, nous pouvons appliquer la règle de la chaine.
\begin{align}
    X_mf = \left. \frac{\pd F}{\pd x^\mu} \right|_{x^\mu=x^\mu(m)} \left. \frac{\pd x^\mu}{\pd \lambda} \right|_{\lambda = 0} (\gamma(\lambda))
\end{align}
En omettant la différence entre la fonction $f$ et sa représentation locale $F$, nous écrirons
\begin{align}
    X_m f = \left. \frac{\td}{\td \lambda} \right|_{\lambda=0} f = \left. \frac{\td x^\mu}{\td \lambda} \right|_{\lambda = 0} \frac{\pd}{\pd x^\mu}f
\end{align}
Comme $f$ est arbitraire, nous trouvons
\begin{equation}
    X_m = \left. \frac{\td x^\mu}{\td \lambda} \right|_{\lambda = 0} \frac{\pd}{\pd x^\mu}
\end{equation}
où les premiers termes sont les composantes du vecteur $X_m$ et les deuxièmes termes $\{\pd_\mu \}_\mu$ forment une base de l'espace $T_m\mathcal{M}$. Remarquons que donc, la dimension de $T_m\mathcal{M}$ est $n$ (la même que l'espace $\mathcal{M}$ correspondant).
\begin{center}
\includegraphics[scale=0.1]{Chapitres/3.Element de géométrie différentielle/Images/vecteurs2.0.jpg}
%\captionof{figure}{}
%\label{Parabolique 2}
\end{center}


\begin{center}
\includegraphics[scale=0.1]{Chapitres/3.Element de géométrie différentielle/Images/dimenson.jpg}
%\captionof{figure}{}
%\label{Parabolique 2}
\end{center}


De manière plus concise, on notera

\begin{equation*}
    X = \frac{\td}{\td \lambda }= \frac{\td x^{\mu}}{\td \lambda } \frac{\partial }{\partial x^{\mu}} = X^{\mu} \partial_{\mu}
\end{equation*}

où nous sous-entendons $x^{\mu} = x^{\mu}( \gamma(\lambda))$. Les vecteurs $X = \pd_\mu$ représentent un déplacement tangent à la direction $x^\mu$. Contrairement à la variété $\mathcal{M}$, qui n'est pas un espace vectoriel dans le cas général, l'espace tangent en est un (exercice !). 
\begin{center}
\includegraphics[scale=0.1]{Chapitres/3.Element de géométrie différentielle/Images/vecteur-fleche.jpg}
%\captionof{figure}{}
%\label{Parabolique 2}
\end{center}
\cutebreak
\subsubsection{Loi de transformation des vecteurs}
Les vecteurs sont indépendants de la base et du choix de la carte locale. Sous changement général de coordonnées (difféomorphisme), les composantes $X_\mu$ d'un vecteur se transforment selon
    \begin{align}
        \boxed{X^\mu \to X^{\mu'} = \frac{\td x^{\mu'}}{\td \lambda} = \frac{\pd x^{\mu'}}{\pd x^\alpha} \frac{\td x^\alpha}{\td \lambda} = \frac{\pd x^{\mu'}}{\pd x^\alpha} X^\alpha}
    \end{align}
Similairement, les vecteurs de base se transforment selon
\begin{equation}
    \pd_\alpha \to \pd_{\alpha'} = \frac{\pd x^\mu}{\pd x^{\alpha'}} \pd_\mu
\end{equation}
de sorte que $X$ soit invariant sous difféomorphisme. Remarquons qu'il s'agit ici simplememnt de la règle de la chaîne. Notons les identités suivantes de la jacobienne $\frac{\pd x^{\mu'}}{\pd x^\alpha}$.
\begin{align}
    \label{eq: id chgmt de coordonnées}
    \boxed{
        \begin{array}{c}
            \dfrac{\pd x^{\mu'}}{\pd x^\alpha}\dfrac{\pd x^{\beta}}{\pd x^{\mu'}} = \displaystyle \delta^\beta_\alpha  \\
            \\
            \dfrac{\pd x^{\alpha}}{\pd x^{\nu'}}\dfrac{\pd x^{\mu'}}{\pd x^\alpha} =\displaystyle\delta\indices*{^{\mu'}_{\nu'}}
        \end{array} 
    }
\end{align}
\begin{exerc}
    Démontrez ces égalités.
\end{exerc}
Pour récapituler, nous avons trouvé qu'un vecteur dans $T_m\mathcal{M}$ peut être vu comme une application
\begin{align}
    X_m : \quad C^\infty(U) &\longrightarrow \quad\R\\
    f \quad&\longmapsto X_m f = \left. X^\mu_m \pd_\mu f \right|_{x^\mu(m)}
\end{align}
où $U$ est un voisinage de $m$. Nous définirons par ailleurs un champ de vecteurs comme la donnée d'un vecteur en tout point $m \in \mathcal{M}$.
\begin{equation}
    \chi: \M \to T\M
\end{equation}
Où $T\mathcal{M}$ est le fibré tangent. Le champ de vecteurs est dit lisse s'il dépend lissement du point $m$ (dans le sens des représentations locales de coordonnées). L'ensemble des champs de vecteurs lisses sur $\mathcal{M}$ est noté $ \Zhe (\mathcal{M})$.
\begin{rmk}
    Nous pouvons à présent justifier l'utilisation de l'union disjointe ($T\M = \bigsqcup_m T_m\M$) dans la définition du fibré tangent : nous souhaitons éviter de confondre les vecteurs appartenant à des espaces tangents en des points différents. En effet, même si deux vecteurs peuvent avoir les mêmes coordonnées dans des cartes distinctes (par exemple $(1,0)$ en $P$ et $(1,0)$ en $Q$), ils représentent des entités géométriquement différentes car ils « vivent » en des points distincts de la variété : la notion de vecteur devient locale à chaque point. La notation en union disjointe formalise cela en considérant chaque vecteur comme un couple $(P,X)$ où $P$ est le point de base et $X$ un élément de $T_P\M$.
\end{rmk}
\subsection{Covecteurs}
Un covecteur $w\in T_m^*\mathcal{M}$ est une forme linéaire sur $T_m\mathcal{M}$ 
\begin{align}
w  :  T_m \mathcal{M} \to \R : X \mapsto w(X)
\end{align}
où pour rappel l'espace $T^*_m \mathcal{M}$ est le dual de $T_m\mathcal{M}$, également appelé l'espace co-tangent. 

\begin{exmp}
    Soit $f\in C^\infty(\mathcal{M})$. La différentielle $\td f$ de $f$ au point $m\in \mathcal{M}$ est un covecteur. Son action est définie par
    \begin{align}
        \left. \td f \right|_m : \quad T_m \mathcal{M} &\longrightarrow \quad  \R \\
        X \quad&\longmapsto \td f (X) = X(f) = \left. X^\mu_m \pd_\mu f \right|_m.
    \end{align}
    On se souviendra qu'une base de l'espace cotangent est donnée par $\{ \td x^\nu\}$. Nous avions également trouvé que la relation entre la base de l'espace tangent et de l'espace co-tangent est
    \begin{equation}
        \td  x^\mu (\pd_\nu) = \delta\indices*{^\mu_\nu}
    \end{equation}
    Suivant notre définition de covecteur, nous pouvons à présent justifier cette relation. En effet, soit $X = \pd_\nu$. Alors
    \begin{equation}
        \td x^\mu \lt \frac{\pd}{\pd x^\nu}\rt \equiv \frac{\pd}{\pd x^\nu}(x^\mu) = \delta\indices*{^\mu_\nu}
    \end{equation}
    Dans une carte locale, $\td f$ se décompose selon
    \begin{equation}
        \td f = \frac{\pd f}{\pd x^\mu} \td x^\mu
    \end{equation}
\end{exmp}
\cutebreak
Montrons à présent que selon notre définition, $w(X)$ est bien scalaire. Dans une carte locale, on a
\begin{equation}
    w(X) = w_\mu \td x^\mu (X^\nu \pd_\nu ) = w_\mu X^\nu \td x^\mu (\pd_\nu) = w_\mu X^\nu \delta\indices*{^\mu_\nu} = w_\mu X^\mu
\end{equation}
Où nous avons utilisé la linéarité de $\td x^\mu$. On conclut que $w(X)$ est bien un scalaire. Sous changements de coordonnées général (difféomorphisme), on trouve donc via l'invariance de $w(X)$ et la loi de transformation de $X$ que
\begin{equation}
    w_\mu X^\mu = w_{\mu'} X^{\mu'} = w_{\mu'} \frac{\pd x^{\mu'}}{\pd x^\mu} X^\mu
\end{equation}
Où on peut donc identifier
\begin{equation}
    w_\alpha = w_{\alpha'} \frac{\pd x^{\alpha'}}{\pd x^\alpha}
\end{equation}
En inversant cette relation via (\ref{eq: id chgmt de coordonnées}), on obtient finalement la loi de transformation sous changement de coordonnées général pour un covecteur

 \begin{equation}
     \boxed{w_\mu \longrightarrow w_{\mu '} = \frac{\partial x^{\alpha }}{\partial x^{\mu '}} w_{\alpha}}
 \end{equation}

Les vecteurs de bases se transforment comme
\begin{equation}
    \td x^{\mu'} = \frac{\partial x^{\mu'}}{\partial x^{\alpha}} \td x^{\alpha}
\end{equation}
\subsection{Les tenseurs généraux}
Comme nous nous intéresserons exclusivement à des espaces à dimension finie, $(T^*_m \mathcal{M})^* \simeq T_m \mathcal{M}$ : les vecteurs peuvent êtres vues comme formes linéaires sur l'espace co-tangent.
\begin{align}
    V: T^*_m \mathcal{M} \to  \R : w  \mapsto V(w) \equiv w(V )
\end{align}
\begin{theoremframe}
    \begin{rap}
        \label{rap:tenseur}
        Un \emph{tenseur} de type $(p,q)$ est une forme multilinéaire 
        \begin{equation}
            \underbrace{T^*_{m}\mathcal{M} \times \cdots \times T^*_{m}\mathcal{M}}_\text{$p$-fois} \times \underbrace{T_{m}\mathcal{M} \times \cdots \times T_{m}\mathcal{M}}_\text{$q$-fois}  \longrightarrow \mathbb{R}
        \end{equation}
    \end{rap}
\end{theoremframe}
\begin{exmp} Donnons quelques exemples\\
    \begin{enumerate}
        \item Un \emph{vecteur} est un objet 
        \begin{equation}
            X:T_m^*\mathcal{M} \longrightarrow \R
        \end{equation}
        et est donc un tenseur $(1,0)$. Dans une carte locale, celui-ci s'écrit $X^\mu \pd_\mu$. Remarquons qu'on a bien $X(\omega) \in \R$.
        \item Un \emph{covecteur} est un objet 
        \begin{equation}
            w:T_m\mathcal{M} \longrightarrow \R
        \end{equation}
        et est donc un tenseur $(0,1)$. Dans une carte locale, celui-ci s'écrit $w_\mu \td x^\mu$.
        \item De manière générale, les composantes d'un tenseur quelconque $(p,q)$ possèdent $p$ indices contravariants et $q$ indices covariants. Dans une carte locale, le tenseur s'écrit
        \begin{equation}
            T = T\indices{^{\alpha_1} ^{\cdots}^{\alpha_p}_{\nu_1}_{\cdots} _{\nu_q}} \partial_{\alpha_{1}}
            \otimes \cdots \otimes \partial_{\alpha_{p}} \otimes \td x^{\nu_{1}}\otimes \cdots\otimes \td x^{\nu_{q}} 
        \end{equation}
        Ses composantes se transforment sous changement de coordonnées général selon
        \begin{equation}
            T\indices{^{\alpha_1} ^{\cdots}^{\alpha_p}_{\nu_1}_{\cdots} _{\nu_q}} \longrightarrow T\indices{^{\alpha'_1} ^{\cdots}^{\alpha'_p}_{\nu'_1}_{\cdots} _{\nu'_q}} = \frac{\partial x^{\alpha'_1}}{\partial x^{\alpha_1}} \cdots\frac{\partial x^{\alpha'_p}}{\partial x^{\alpha_p}}\frac{\partial x^{\nu_1}}{\partial x^{\nu'_1}}\cdots\frac{\partial x^{\nu_q}}{\partial x^{\nu'_q}}\, T\indices{^{\alpha_1} ^{\cdots}^{\alpha_p}_{\nu_1}_{\cdots} _{\nu_q}}
        \end{equation}
    \end{enumerate}
\end{exmp}
\section{Les p-formes différentielles}
Nous souhaitons pouvoir définir une structure différentielle sur notre variété, c'est-à-dire construire une manière de différencier un tenseur quelconque sur notre variété, ce qui nous permettrait d'étudier des systèmes dynamiques (i.e. des équations différentielles). Or, un problème apparaît lorsqu'on s'intéresse à la quantité $\pd_\mu w_\nu$. Sous changement général de coordonnées, cette quantité varie selon
\begin{align}
\nonumber
    \label{eq: not a tensor}
    \pd_\mu w_\nu \to \pd_{\mu'} w_{\nu'} &= \frac{\pd x^\mu}{\pd x^{\mu'}} \pd_\mu \lt \frac{\pd x^\nu}{\pd x^{\nu'}} w_\nu \rt\\
    \nonumber
    & = \frac{\pd x^\mu}{\pd x^{\mu'}} \frac{\pd x^\nu}{\pd x^{\nu'}} \pd_\mu w_\nu + \frac{\pd x^\mu}{\pd x^{\mu'}} w_\nu \frac{\pd}{\pd x^\mu}  \frac{\pd x^\nu}{\pd x^{\nu'}}\\
    &= \frac{\pd x^\mu}{\pd x^{\mu'}} \frac{\pd x^\nu}{\pd x^{\nu'}} \pd_\mu w_\nu + w_\nu  \frac{\pd^2 x^\nu}{\pd x^{\mu'}\pd x^{\nu'}}
\end{align}
Ainsi, comme sous changement de coordonnées général (difféomorphisme), le second terme ne s'annule pas, $\pd_\mu w_\nu$ \emph{n'est pas un tenseur}. La relation (\ref{eq: not a tensor}) motivera la notion de \emph{p-forme différentielle}.
\begin{theoremframe}
    \begin{defi}
        Une $p$-forme différentielle est un tenseur de type $(0,p)$ totalement antisymétrique.
    \end{defi}
\end{theoremframe}
\begin{propri}
    L'ensemble des p-formes en un point $m\in \mathcal{M}$, noté $\displaystyle\Omega\indices*{^p_m}$ forme un espace vectoriel.
\end{propri}

Par exemple, une 2-forme est une application telle que 
\begin{equation}
    F: T_m\mathcal{M}\times T_m\mathcal{M}\rightarrow \R: (X, Y)\mapsto F(X, Y) = -F(Y, X)
\end{equation}

En composantes\footnote{Nous omettrons dans la suite de préciser la validité uniquement dans une carte locale.}, cette 2-forme s'écrit
\begin{equation}
    F(\partial_{\alpha}, \partial_{\beta}) = F_{\mu \nu}\, \td x^{\mu}\otimes \td x^{\nu}(\partial_{\alpha}, \partial_{\beta}) = F_{\mu \nu} \delta^{\mu}_{\nu}\,\delta^{\nu}_{\beta } = F_{\alpha \beta}
\end{equation}
où $F_{\alpha \beta}$ sont les composantes du tenseurs $F$. Comme $F$ est antisymétrique alors $F(\partial_{\alpha}, \partial_{\beta}) = -F(\partial_{\beta}, \partial_{\alpha}) = -F_{ \beta \alpha}$. L'antisymétricité en les arguments implique donc l'antisymétricité en les indices.
\begin{rmk}
    Notons les espaces suivants :
    \begin{enumerate}
        \item Une 1-forme est un covecteur : $\Omega_m^1 = T_m^*\mathcal{M}$
        \item Une 0-forme est une fonction : $\Omega_m^0 = \left. C^\infty(\mathcal{M}) \right|_m$
    \end{enumerate}
\end{rmk}

\subsection{Wedge product}

  Introduisons la notion de \emph{wedge-product}, généralisant le produit vectoriel à une dimension quelconque.
\begin{theoremframe}
    \begin{defi}
        Le wedge-product est défini comme l'opération 
        \begin{equation}
        \wedge : T_m\mathcal{M} \times T_m\mathcal{M} \to T_m \mathcal{M} \otimes T_m\mathcal{M}
        \end{equation}
        telle que
        \begin{equation}
            \td x^{\mu_1}\wedge \cdots\wedge \td x^{\mu_r} = \sum_{\sigma \in S_r} \sgn(\sigma) \td x^{\mu_{\sigma(1)}} \otimes \cdots \otimes \td x^{\mu_{\sigma(r)}}
        \end{equation}
    \end{defi}
\end{theoremframe}
\begin{exmp}
    Donnons quelques exemples de \emph{wedge-product} :
    \begin{enumerate}
        \item Si $r=2$, alors
        \begin{equation}
            \td x^{\mu}\wedge \td x^{\nu} = \td x^{\mu}\otimes \td x^{\nu} - \td x^{\nu}\otimes \td x^{\mu}
        \end{equation}
        \item Si $r=3$, alors
        \begin{align*}
            \td x^{\alpha}\wedge \td x^{\beta}\wedge \td x^{\gamma} =&+\quad \td x^{\alpha}\otimes \td x^{\beta}\otimes \td x^{\gamma} \quad - \quad\td x^{\beta}\otimes\td x^{\alpha}\otimes \td x^{\gamma}\\
            &+ \quad \td x^{\beta}\otimes \td x^{\gamma}\otimes \td x^{\alpha} \quad - \quad \td x^{\alpha}\otimes \td x^{\gamma}\otimes \td x^{\beta} \\
            &+\underbrace{
            \td x^{\gamma}\otimes \td x^{\alpha}\otimes \td x^{\beta}}_{\text{permutations cycliques de $\mathrm{Id}_3$}} - \, \quad
            \td x^{\gamma}\otimes \td x^{\beta}\otimes \td x^{\alpha}
\end{align*}
    \end{enumerate}
\end{exmp}
Dans le dernier exemple, nous avons bien 6 termes car il y a $3! = 6$ éléments dans $S_3$.
\begin{exerc}
    Montrez que les $\{ \td x^{\mu}\wedge \td x^{\nu} \}_{\mu<\nu}$ forment bien une base de $\Omega_m^2$.
\end{exerc}
Dans cette base, une p-forme s'écrit
\begin{equation}
    \omega = \frac{1}{p!}\omega_{\mu_1 \cdots \mu_p}\,\td x^{\mu_1}\wedge \cdots \wedge \td x^{\mu_p} =\frac{1}{p!}\omega_{[\mu_1 \cdots \mu_p]}\,\td x^{\mu_1}\wedge \cdots \wedge \td x^{\mu_p}
\end{equation}
La dimension de cette espace $\Omega^{p}_m$ correspond au nombre d'éléments de cette base en sachant que tout les indices doivent être différent les uns des autres par antisymétrie du \emph{wedge-product}. Si $\dim \mathcal{M} = k$, alors le nombre dont on peut prendre $p$ éléments parmi les $m$ est 
\begin{equation*}
{p \choose k} = \frac{k!}{p!(k-p)} = \dim(\Omega^{p}_m)
\end{equation*}
Par exemple, pour $\dim \mathcal{M} = 3$, on trouve $\dim \Omega_m^2 = {2\choose 3} = 3$. Une base de cet espace est
\begin{equation}
    \{\td x \wedge \td y , \td x \wedge \td y , \td y \wedge \td z\}
\end{equation} 
Remarquons qu'en conséquence,il est clair qu'il n'existe pas de p-formes pour $p>\dim \mathcal{M}$.


\subsection{Dérivée extérieur}
On peut définir un opérateur différentiel sur les p-formes.
\begin{theoremframe}
    \begin{defi}
        La \emph{dérivée extérieure} d'un champ de p-formes est une application
        \begin{equation}
            \td :\Omega^p \to \Omega^{p+1}:\omega \mapsto \td \omega 
        \end{equation}
    \end{defi}
\end{theoremframe}
Sur une p-forme 
\begin{equation}
     \omega = \frac{1}{p!}\omega_{\mu_1 \cdots \mu_p}\, \td x^{\mu_1}\wedge \cdots \wedge \td x^{\mu_p}
\end{equation}
la dérivée extérieure agit comme
\begin{align}
    \td \omega  &\equiv \frac{1}{p!}\partial_{[{\nu }}\omega _{\mu_1 \cdots \mu_p]}\td x^{\nu}\wedge \td x^{\mu}\\
    &= \frac{1}{(p+1)!}(\td \omega )_{\nu \mu_1 \cdots \mu_p}\td x^{\nu}\wedge \td x^{\mu_1}\cdots \td x^{\mu_p}
\end{align}
Où la première ligne est la définition de la dérivée extérieure, et la seconde ligne vient du fait que $\td \omega $ est une (p+1)-forme. On trouve donc que les composantes de $\td \omega $ s'écrivent
\begin{equation}
    (\td \omega )_{\nu \mu_1 \cdots \mu_p} = (p+1)\partial_{[{\nu }}\omega _{\mu_1 \cdots \mu_p]}
\end{equation}
\begin{rmk}
    Si $f $ est une 0-forme, alors $\td f$ est une 1-forme qui vaut
    \begin{equation} 
    \td f = \frac{\partial f}{\partial x^{\mu}}\td x^{\mu}
    \end{equation}
    qui est la différentielle usuelle.
\end{rmk}
\cutebreak
L'utilité de la dérivée extérieur est que $\td \omega $ reste un tenseur. Elle est donc valable dans tout système de coordonnées.
\begin{exerc}
    Montrez à l'aide de (\ref{eq: not a tensor}) que si $\omega $ est une p-forme, alors $\td \omega $ se transforme comme un tenseur $(0,p+1)$.
\end{exerc}
Remarquons qu'en considérant uniquement les transformations de Poincaré (entre référentiels inertiels)
\begin{equation}
    x^\mu \to x^{\mu'} = \Lambda\indices{^{\mu'}_\!_\alpha}\,x^\alpha +a^{\mu'}
\end{equation}
les objets de type $\pd_\mu \omega _\nu$ se transforme de manière covariante, car le deuxième terme de (\ref{eq: not a tensor}) s'annule nécessairement par linéarité de la transformation. Bien que ce n'est plus le cas sous changement général de coordonnées, la covariance reste assuré lorsqu'on se restreint à des $p$-formes. Par exemple, si une équation permet de se réécrire dans la forme
\begin{equation}
    \td F = 0
\end{equation}
alors cette équation reste valable dans tout référentiel. Nous avons ainsi trouvé une manière d'écrire une équation de manière covariante sous changement général de coordonnées. Néanmoins, celle-ci n'est valable que dans le cas des $p$-formes. Dans le chapitre suivant, nous allons construire une théorie différentielle définie pour tout tenseur : la dérivée covariante.\\
\\
Terminons cette section par une introduction informelle à la théorie de cohomologie.
\begin{theoremframe}
    \begin{defi}
        Une $p$-forme $\omega $ est dite \emph{fermée} si $\td \omega  = 0$. Elle est dite \emph{exacte} s'il existe une $(p-1)$-forme $\eta$ telle que $\omega =\td \eta$.
    \end{defi}
\end{theoremframe}
Le lien entre ces deux définitions est donné par la prorpiété suivante
\begin{theoremframe}
    \begin{propri}
        Une $p$-forme exacte est fermée. Autrement dit, l'opérateur
        \begin{equation}
            \td ^2: \Omega_m^p \to \Omega_m^{p+2}:\omega  \mapsto \td (\td \omega )
        \end{equation}
        est identiquement nul : $\td^2=0$.
    \end{propri}
\end{theoremframe}
\begin{proof}
    Soit une $p$-forme arbitraire
    \begin{align}
        &\omega  = \frac{1}{p!}\omega _{\mu_1 \cdots \mu_p}\,\td x^{\mu_1}\wedge \cdots \wedge \td x^{\mu_p}
        \intertext{Alors sa dérivée extérieure s'écrit}
         & \td \omega  = \frac{1}{p!}\partial_{[ \nu}\omega _{\mu_1 \cdots \mu_p]}\,\td x^{\nu} \wedge \td x^{\mu_1}\wedge \cdots \wedge \td x^{\mu_p}
         \intertext{Et si on applique la dérivée extérieure une deuxième fois}
        & \td(\td\omega ) = \frac{1}{p!}\partial_{[\beta}\partial_{\nu}\omega _{\mu_1 \cdots \mu_p]}\,\td x^{\beta}\wedge \td x^{\nu}\wedge \td x^{\mu_1}\wedge \cdots \wedge \td x^{\mu_p}= 0
    \end{align}
    par contraction antisymétrique en $\td x^\beta \wedge \td x^\nu$ avec $\pd_\beta \pd_\nu$ symétrique, cette expression s'annule. En effet, en considérant uniquement le terme
    \begin{equation}
        A = \partial_{\beta}\partial_{\nu}f \td x^{\beta}\wedge \td x^{\nu}
    \end{equation}
    Comme le \emph{wedge-product} est antisymétrique, ce terme vaut
    \begin{align}
        -\partial_{\beta}\partial_{\nu}f \td x^{\nu}\wedge \td x^{\beta}
    \end{align}
    Comme la dérivée partielle est symétrique, nous pouvons les permuter librement. En renommant nos indices (qui sont tous sommés) $\beta \leftrightarrow \nu$, on trouve finalement
    \begin{align}
        A=-\partial_{\beta}\partial_{\nu}f dx^{\beta}\wedge dx^{\nu}
    \end{align}
    Comme $A=-A$, on conclut que ce terme s'annule bien, ce qui conclut la preuve.
\end{proof}
Dans la suite du cours, nous considérerons souvent des contractions symétriques-antisymétriques, qui s'annuleront systématiquement comme montré ci-dessus. Nous ne redémontrerons pas ce résultat élémentaire.

Remarquons que l'implication inverse est, en général, fausse, c'est-à-dire que toute $p$-forme fermée n'est pas nécessairement exacte. Il est néanmoins possible d'en formuler une version moins forte, qu'on ne va ici que mentionner sur le passage : toute $p$-forme fermée est \emph{localement} exacte. Si $\omega $ est fermée en $m\in\mathcal{M}$, alors il existe un voisinage de $m$ sur lequel $\omega $ est exacte.
\begin{exerc}
    Montrez que l'ensemble des $p$-formes fermées (resp. exactes) sur $\mathcal{M}$ est un espace vectoriel. Il est noté $Z^p(\mathcal{M})$ (resp. $B^p(\mathcal{M})$).
\end{exerc}
En notant la dérivée extérieure sur une $p$-forme $\td^{(p)}$, on peut noter que
\begin{align}
    Z^p(\mathcal{M}) &= \mathrm{Ker} \, \td^{(p)}\\
    B^p(\mathcal{M}) &= \mathrm{Im} \, \td^{(p-1)}
\end{align}
Dans cette notation, la propriété précédente se formule $B^p \subseteq Z^p$. Cette reformulation permet de donner sens à la définition suivante :
\begin{theoremframe}
    \begin{defi}
        On définit le $p$-ième groupe de cohomologie de \emph{de Rham} par le l'espace vectoriel quotient 
        \begin{equation}
            H^p(\mathcal{M}) = \frac{Z^p(\mathcal{M})}{B^p(\mathcal{M})}
        \end{equation}
    \end{defi}
\end{theoremframe}
On peut interpréter ce groupe comme une mesure de l'obstruction pour une $p$-forme d'être exacte dans la variété $\mathcal{M}$. Les éléments de ce groupe sont des classes d'équivalences de $p$-formes définies comme :
\begin{equation}
    \omega  \sim \omega ' \iff \exists \eta \in \Omega^{p-1}, \omega  = \omega ' + \td \eta
\end{equation}
Autrement dit, $\omega $ et $\omega '$ ne diffèrent que d'un élément de $B^p$. Lorsque $\mathcal{M}$ est tel que toute forme fermée est exacte, on a $H^p(\mathcal{M})=0$ (car alors toute $p$-forme est équivalente à la $p$-forme nulle). De manière générale, la dimension des classes de cohomologie (qui requiert usuellement la résolutions d'équations différentielles) ne dépend que de la topologie de $\mathcal{M}$. Par exemple, si la variété est homéomorphe à $\R^n$, alors $H^p(\mathcal{M}) = 0$ pour tout $p>0$.\\
Nous invitons le lecteur à considérer le chapitre 6 de [Source].

